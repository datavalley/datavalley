<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Data Valley</title>
  <link href="//atom.xml" rel="self"/>
  <link href=""/>
  <updated>2016-07-15T11:21:02+08:00</updated>
  <id></id>
  <author>
    <name>小z</name>
  </author>

  
  <entry>
    <title>Hive优化之控制map数和reduce数[转]</title>
    <link href="/2016/07/15/hive%E4%BC%98%E5%8C%96%E4%B9%8B%E6%8E%A7%E5%88%B6hive%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84map%E6%95%B0%E5%92%8Creduce%E6%95%B0/"/>
    <updated>2016-07-15T00:00:00+08:00</updated>
    <id>/2016/07/15/hive优化之控制hive任务中的map数和reduce数</id>
    <content type="html">原文链接:hive优化之------控制hive任务中的map数和reduce数

1. 控制hive任务中的map数:

1.1 通常情况下，作业会通过input的目录产生一个或者多个map任务。

主要的决定因素有： input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；

1.2 举例：

a)假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b)假设input...</content>
  </entry>
  
  <entry>
    <title>HCataLog学习笔记</title>
    <link href="/2016/06/22/HCataLog%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <updated>2016-06-22T00:00:00+08:00</updated>
    <id>/2016/06/22/HCataLog学习笔记</id>
    <content type="html">1 HCatInputFormat

HCatInputFormat提供一些mapreduce api来读取表

1.1 setInput()

setInput() 用于设置MR的输入，例如：输入的表名、表所在的数据库以及数据的分区。setInput() 方法有以下几种定义：
  /**
   * Initializes the input with a null filter.
   * See {@link #setInput(org.apache.hadoop.conf.Configuration, String, String, String)}
   */
  public ...</content>
  </entry>
  
  <entry>
    <title>Java反射学习笔记</title>
    <link href="/2016/06/04/Java%E5%8F%8D%E5%B0%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
    <updated>2016-06-04T00:00:00+08:00</updated>
    <id>/2016/06/04/Java反射学习笔记2</id>
    <content type="html">0. 准备

reflection.Person.java文件：
package reflection;

/**
 * Person接口
 */
public interface Person {
    public void sayHello(String otherName);
}

reflection.Student.java文件：
package reflection;

public class Student implements Person {
    private String name;
    private int age;

    public Stu...</content>
  </entry>
  
  <entry>
    <title>Java反射学习笔记</title>
    <link href="/2016/06/04/Java%E5%8F%8D%E5%B0%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <updated>2016-06-04T00:00:00+08:00</updated>
    <id>/2016/06/04/Java反射学习笔记</id>
    <content type="html">Java的反射机制，允许程序在运行时获取类的任何信息：包括类名、父类、实现的接口、参数列表、构造函数列表、方法列表以及属性和方法的修饰符等等。Java的反射机制使用过一组Java类实现的，包括：Class, Method, Constructor以及Modifier等。

下面的一个例子展示了Java反射机制的基本概念和用法（仅仅是学习笔记）。参考文献部分列出了比较好的博客，请自行参阅。
package reflection.data;

import java.lang.reflect.*;

/**
 * Created by zmz on 16/5/26.
 */


interf...</content>
  </entry>
  
  <entry>
    <title>Java设计模式之代理模式[转]</title>
    <link href="/2016/05/29/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F-%E8%BD%AC/"/>
    <updated>2016-05-29T00:00:00+08:00</updated>
    <id>/2016/05/29/Java设计模式之代理模式[转]</id>
    <content type="html">原文：代理模式原理及实例讲解

东汉末年，大将军何进引董卓入京，想借西北王的军队对抗阉党，无奈自己先被阉党做掉，而后造成巨变，导致诸侯并起，最终形成三国鼎立局面。汉献帝即位后，初平三年（公元 192 年），治中从事毛玠向曹操建议“奉天子以令不臣”，曹操采纳了他的建议，迎接汉献帝来到许昌。汉献帝刘协在许都没有实际的权利，曹操不断地诛除公卿大臣，不断地集军政大权于一身。建安元年八月，曹操进驻洛阳，立刻趁张杨、杨奉兵众在外，赶跑了韩暹，接着做了三件事：杀侍中台崇、尚书冯硕等，谓“讨有罪”；封董承、伏完等，谓“赏有功”；追赐射声校尉沮俊，谓“矜死节”。然后在第九天趁他人尚未来得及反应的情况下，迁...</content>
  </entry>
  
  <entry>
    <title>Scala学习笔记之模式匹配</title>
    <link href="/2016/04/12/scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/"/>
    <updated>2016-04-12T00:00:00+08:00</updated>
    <id>/2016/04/12/scala学习笔记之模式匹配</id>
    <content type="html">在Java中，我们使用switch...case对一个变量进行匹配，根据匹配结果进行不同的运算。相对应的，scala使用match...case来完成模式匹配的功能，而且scala的模式匹配功能更加强大。

1. 入门小栗子

我们先来看一个小栗子，以便对scala的模式匹配有一个直观的印象。


例1

def matchTest(ch: Char): String = ch match {
    case &amp;#39;j&amp;#39; =&amp;gt; &amp;quot;Java&amp;quot;
    case &amp;#39;s&amp;#39; =&amp;gt; &amp;quot;Scala&amp;quot;
    case ...</content>
  </entry>
  
  <entry>
    <title>Scala学习笔记之基础知识</title>
    <link href="/2016/04/12/scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <updated>2016-04-12T00:00:00+08:00</updated>
    <id>/2016/04/12/scala学习笔记之基础知识</id>
    <content type="html">Option
</content>
  </entry>
  
  <entry>
    <title>Spark学习笔记之之RDD编程</title>
    <link href="/2016/04/11/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8BRDD%E7%BC%96%E7%A8%8B/"/>
    <updated>2016-04-11T00:00:00+08:00</updated>
    <id>/2016/04/11/spark学习笔记之RDD编程</id>
    <content type="html">1. RDD简介

RDD，全称Resilient Distributed Datasets（弹性分布式数据集），是Spark最为核心的概念，是Spark对数据的抽象。RDD是分布式的元素集合，每个RDD只支持读操作，且每个RDD都被分为多个分区存储到集群的不同节点上。除此之外，RDD还允许用户显示的指定数据存储到内存和磁盘中。

对RDD的操作，从类型上也比较简单，包括：创建RDD、转化已有的RDD以及在已有RDD的基础上进行求值。

现在理解这些概念可能比较抽象。简言之，我们可以把RDD看成是spark对数据的抽象，例如：可以为集群上的一个文件README.md创建一个RDD对象li...</content>
  </entry>
  
  <entry>
    <title>Scala学习笔记之implicit隐式转换</title>
    <link href="/2016/04/07/scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8Bimplicit/"/>
    <updated>2016-04-07T00:00:00+08:00</updated>
    <id>/2016/04/07/scala学习笔记之implicit</id>
    <content type="html">在编程语言中，经常会遇到类型转换的情形，有些类型转换是显式的，而有些类型转换是隐式的，例如：2 + 3.14f，这里编译器就会将整型隐式的转换为浮点型，然后再完成求和运算。

在Scala中，使用implicit完成隐式类型转换，并且允许用户对类型转换进行自定义：根据用户的意愿，完成类型之间的转换。

implicit有两个用途：


完成隐式的类型转换
在不改变已有类型定义的前提下，为类添加新的方法


1. 隐式转换

在scala中，隐式转换是通过implicit实现的，我们先来看一个小栗子：


例1

scala&amp;gt; implicit def foo(s: String):...</content>
  </entry>
  
  <entry>
    <title>Scala学习笔记</title>
    <link href="/2016/03/07/scala%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <updated>2016-03-07T00:00:00+08:00</updated>
    <id>/2016/03/07/scala学习笔记</id>
    <content type="html">现在开头：


这只是份简单的笔记，本不应敝履自珍，但手痒难耐还是写点废话在前面。
书籍浩如烟海，技术方面的书也汗牛充栋，可惜我们的阅读速度和理解力、记忆力太有限，往往费力学懂的知识转眼就变得非常陌生，“博闻强志、过目不忘”者毕竟罕见。对于大部分人来说，即便昔日信手拈来的东西，时间久了也会毫无头绪。所以知识不在于曾经学过多少，而在于你记住并还能运用多少。

“好记性不如烂笔头”——近年来我慢慢习惯把费力学习的东西都做一个笔记，一是在学习的过程中加深印象，毕竟技术学习不同于欣赏娱乐大片和浏览娱乐新闻看个过眼烟云；二是便于学而“时习之”，书上的东西一般是针对不同技术背景的读者，有很多作者费力...</content>
  </entry>
  
  <entry>
    <title>Hive之执行计划</title>
    <link href="/2016/02/16/Hive%E4%B9%8B%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92/"/>
    <updated>2016-02-16T00:00:00+08:00</updated>
    <id>/2016/02/16/Hive之执行计划</id>
    <content type="html">EXPLAIN
SELECT
  datekey,
  count(*)
FROM topic.deal_date
WHERE datekey = 20160201
GROUP BY datekey
;

上述SQL的执行计划为：
STAGE DEPENDENCIES:   -- 上述SQL的执行阶段信息，在这里，Stage-1对应一个mapreduce job
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operat...</content>
  </entry>
  
  <entry>
    <title>Hive之COUNT DISTINCT优化</title>
    <link href="/2016/02/15/Hive%E4%B9%8BCOUNT-DISTINCT%E4%BC%98%E5%8C%96/"/>
    <updated>2016-02-15T00:00:00+08:00</updated>
    <id>/2016/02/15/Hive之COUNT DISTINCT优化</id>
    <content type="html">COUNT(DISTINCT xxx)在hive中很容易造成数据倾斜。针对这一情况，网上已有很多优化方法，这里不再赘述。

但有时，“数据倾斜”又几乎是必然的。我们来举个例子：

假设表detail_sdk_session中记录了访问某网站M的客户端会话信息，即：如果用户A打开app客户端，则会产生一条会话信息记录在该表中，该表的粒度为“一次”会话，其中每次会话都记录了用户的唯一标示uuid，uuid是一个很长的字符串，假定其长度为64位。现在的需求是：每天统计当月的活用用户数——“月活跃用户数”（当月访问过app就为活跃用户）。我们以2016年1月为例进行说明，now表示当前日期。

...</content>
  </entry>
  
  <entry>
    <title>MAC挂载NTFS硬盘或U盘</title>
    <link href="/2016/02/14/MAC%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98%E6%88%96U%E7%9B%98/"/>
    <updated>2016-02-14T00:00:00+08:00</updated>
    <id>/2016/02/14/MAC挂载NTFS硬盘或U盘</id>
    <content type="html">在平时工作和生活中，经常需要在windows和mac之间进行文件拷贝，对于较小的文件可以利用QQ的文件传输功能完成。但是对于较大的文件来说，由于网络带宽的限制，使用QQ等借助网络进行传输的软件就变的得不偿失。这时候就需要借助U盘或者硬盘来进行文件的拷贝。

现在大多数情况下，U盘或者硬盘都是在windows进行格式化得，文件系统的格式一般为NTFS。mac对于NTFS可以直接进行读取操作，但对于写操作，就需要额外的手段来支持了。

我也曾经尝试过使用一些软件，使得mac可以直接对NTFS的U盘进行写操作，但过了试用期就无法使用了。又不想买正版（穷。。。。），怎么办呢？？

这怎么能难住程...</content>
  </entry>
  
  <entry>
    <title>Hive之mac环境中的Java路径问题</title>
    <link href="/2015/10/26/Hive%E4%B9%8Bmac%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84Java%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/"/>
    <updated>2015-10-26T00:00:00+08:00</updated>
    <id>/2015/10/26/Hive之mac环境中的Java路径问题</id>
    <content type="html">1.问题描述

在mac系统上，搭建了hadoop和hive运行环境。在hive命令行下，可以执行简单的SELECT、SHOW TABLES等操作。但是执行连接时，就会报错，报错信息如下：
Query ID = zmz_20151026191653_ea580dc6-ec94-4fcb-8058-b32f8045acfa
Total jobs = 1
15/10/26 19:16:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-ja...</content>
  </entry>
  
  <entry>
    <title>Hive:分区字段的异常字符</title>
    <link href="/2015/10/25/Hive%E4%B9%8B%E5%88%86%E5%8C%BA%E5%AD%97%E6%AE%B5%E7%9A%84%E5%BC%82%E5%B8%B8%E5%AD%97%E7%AC%A6/"/>
    <updated>2015-10-25T00:00:00+08:00</updated>
    <id>/2015/10/25/Hive之分区字段的异常字符</id>
    <content type="html">1. 问题描述

hive支持动态分区技术，可以将SELECT出来的数据根据分区字段的取值动态的插入到目标分区中。例如：
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

INSERT OVERWRITE TABLE target_table_name PARTITION(partition_field_name)
SELECT
  ...
  partition_field_name
FROM source_table_name
WHERE ...

注意，使用动...</content>
  </entry>
  
  <entry>
    <title>Hive:JOIN及JOIN优化</title>
    <link href="/2015/10/25/Hive%E4%B9%8BJOIN%E5%8F%8AJOIN%E4%BC%98%E5%8C%96/"/>
    <updated>2015-10-25T00:00:00+08:00</updated>
    <id>/2015/10/25/Hive之JOIN及JOIN优化</id>
    <content type="html">1. Join的基本原理

大家都知道，Hive会将所有的SQL查询转化为Map/Reduce作业运行于Hadoop集群之上。在这里简要介绍Hive将Join转化为Map/Reduce的基本原理（其它查询的原理请参考这里）。

假定有user和order两张表，分别如下：

user表：



sid
name



1
apple


2
orange



order表：



uid
orderid



1
1001


1
1002


2
1003



现在想做student和sc两张表上的连接操作：
SELECT
  u.name,
  o.orderid
FROM us...</content>
  </entry>
  
  <entry>
    <title>Hive:源码解析之本地环境搭建</title>
    <link href="/2015/10/16/Hive%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <updated>2015-10-16T00:00:00+08:00</updated>
    <id>/2015/10/16/Hive源码解析之本地环境搭建</id>
    <content type="html">在使用了hive一段时间后，对简单的hive基本原理和操作有了一些了解。hive构建与hadoop之上，是非常流行的数据仓库工具。hive将结构化的数据映射为数据文件存储在hdfs上，并提供SQL查询功能对数据进行检索和处理。最终，hive将SQL查询转换为MapReduce任务。

面对hive强大的功能，总有一些冲动想搞清其“庐山真面目”，弄懂其运行的基本流程和原理。于是开启了hive源码的学习之路。

在这里将学习过程中的经历和心得记录下来，希望对想学习hive的同学提供一些帮助。

如果想学习hive的源码，个人感觉最好的方式就是在本地机器上搭建hive环境，以代码跟踪的方式学习...</content>
  </entry>
  
  <entry>
    <title>Hive源码解析之Hive基本框架和执行入口[转]</title>
    <link href="/2015/10/16/Hive%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8BHive%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E5%92%8C%E6%89%A7%E8%A1%8C%E5%85%A5%E5%8F%A3/"/>
    <updated>2015-10-16T00:00:00+08:00</updated>
    <id>/2015/10/16/Hive源码解析之Hive基本框架和执行入口</id>
    <content type="html">原文：http://segmentfault.com/a/1190000002766035

1.Hive简介

在介绍Hive的框架和执行流程之前，这里首先对Hive进行简要的介绍。

Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据抽取，转化，加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为Hive QL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建...</content>
  </entry>
  
  <entry>
    <title>OLAP</title>
    <link href="/2015/10/16/olap/"/>
    <updated>2015-10-16T00:00:00+08:00</updated>
    <id>/2015/10/16/olap</id>
    <content type="html">1.介绍

OLAP（On-Line Analytical Processing），译为联机分析处理，权威的定义为：OLAP是使分析人员、管理人员或执行人员能够从多种角度对从原始数据中转化出来的、能够真正为用户所理解的、并真实反映企业维特性的信息进行快速、一致、交互地存取，从而获得对数据的更深入了解的一类软件技术。

目前数据处理大致可以分为两大类：OLAP和OLTP(On-Line Transaction Processing，联机事务处理)。OLTP的出现早于OLAP。

OLTP是传统数据库的主要应用，面向当前的数据，主要是基本的、日常的事务处理。随着数据库技术的广泛应用，企业产生...</content>
  </entry>
  
  <entry>
    <title>Jekyll格式说明!</title>
    <link href="/2015/10/14/hello-jekyll/"/>
    <updated>2015-10-14T00:00:00+08:00</updated>
    <id>/2015/10/14/hello-jekyll</id>
    <content type="html">1. 标题与文字格式

标题
# 测试 h1
## 测试 h2
### 测试 h3
#### 测试 h4
##### 测试 h5
###### 测试 h6

效果：

测试 h1

测试 h2

测试 h3

测试 h4

测试 h5

测试 h6

文字格式
**这是文字粗体格式**
*这是文字斜体格式*
~~在文字上添加删除线~~

这是文字粗体格式
这是文字斜体格式
在文字上添加删除线

2. 列表

无序列表
* 项目1
* 项目2
* 项目3


项目1
项目2
项目3


有序列表
1. 项目1
2. 项目2
3. 项目3
   * 项目1
   * 项目2


项目1
项目...</content>
  </entry>
  
</feed>
