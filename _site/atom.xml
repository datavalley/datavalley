<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Data Valley</title>
  <link href="//atom.xml" rel="self"/>
  <link href=""/>
  <updated>2016-03-07T11:42:30+08:00</updated>
  <id></id>
  <author>
    <name>小z</name>
  </author>

  
  <entry>
    <title>Hive之执行计划</title>
    <link href="/2016/02/16/Hive%E4%B9%8B%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92/"/>
    <updated>2016-02-16T00:00:00+08:00</updated>
    <id>/2016/02/16/Hive之执行计划</id>
    <content type="html">EXPLAIN
SELECT
  datekey,
  count(*)
FROM topic.deal_date
WHERE datekey = 20160201
GROUP BY datekey
;

上述SQL的执行计划为：
STAGE DEPENDENCIES:   -- 上述SQL的执行阶段信息，在这里，Stage-1对应一个mapreduce job
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operat...</content>
  </entry>
  
  <entry>
    <title>Hive之COUNT DISTINCT优化</title>
    <link href="/2016/02/15/Hive%E4%B9%8BCOUNT%20DISTINCT%E4%BC%98%E5%8C%96/"/>
    <updated>2016-02-15T00:00:00+08:00</updated>
    <id>/2016/02/15/Hive之COUNT DISTINCT优化</id>
    <content type="html">COUNT(DISTINCT xxx)在hive中很容易造成数据倾斜。针对这一情况，网上已有很多优化方法，这里不再赘述。

但有时，“数据倾斜”又几乎是必然的。我们来举个例子：

假设表detail_sdk_session中记录了访问某网站M的客户端会话信息，即：如果用户A打开app客户端，则会产生一条会话信息记录在该表中，该表的粒度为“一次”会话，其中每次会话都记录了用户的唯一标示uuid，uuid是一个很长的字符串，假定其长度为64位。现在的需求是：每天统计当月的活用用户数——“月活跃用户数”（当月访问过app就为活跃用户）。我们以2016年1月为例进行说明，now表示当前日期。

...</content>
  </entry>
  
  <entry>
    <title>MAC挂载NTFS硬盘或U盘</title>
    <link href="/2016/02/14/MAC%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98%E6%88%96U%E7%9B%98/"/>
    <updated>2016-02-14T00:00:00+08:00</updated>
    <id>/2016/02/14/MAC挂载NTFS硬盘或U盘</id>
    <content type="html">在平时工作和生活中，经常需要在windows和mac之间进行文件拷贝，对于较小的文件可以利用QQ的文件传输功能完成。但是对于较大的文件来说，由于网络带宽的限制，使用QQ等借助网络进行传输的软件就变的得不偿失。这时候就需要借助U盘或者硬盘来进行文件的拷贝。

现在大多数情况下，U盘或者硬盘都是在windows进行格式化得，文件系统的格式一般为NTFS。mac对于NTFS可以直接进行读取操作，但对于写操作，就需要额外的手段来支持了。

我也曾经尝试过使用一些软件，使得mac可以直接对NTFS的U盘进行写操作，但过了试用期就无法使用了。又不想买正版（穷。。。。），怎么办呢？？

这怎么能难住程...</content>
  </entry>
  
  <entry>
    <title>Hive之mac环境中的Java路径问题</title>
    <link href="/2015/10/26/Hive%E4%B9%8Bmac%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84Java%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/"/>
    <updated>2015-10-26T00:00:00+08:00</updated>
    <id>/2015/10/26/Hive之mac环境中的Java路径问题</id>
    <content type="html">1.问题描述

在mac系统上，搭建了hadoop和hive运行环境。在hive命令行下，可以执行简单的SELECT、SHOW TABLES等操作。但是执行连接时，就会报错，报错信息如下：
Query ID = zmz_20151026191653_ea580dc6-ec94-4fcb-8058-b32f8045acfa
Total jobs = 1
15/10/26 19:16:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-ja...</content>
  </entry>
  
  <entry>
    <title>Hive:分区字段的异常字符</title>
    <link href="/2015/10/25/Hive%E4%B9%8B%E5%88%86%E5%8C%BA%E5%AD%97%E6%AE%B5%E7%9A%84%E5%BC%82%E5%B8%B8%E5%AD%97%E7%AC%A6/"/>
    <updated>2015-10-25T00:00:00+08:00</updated>
    <id>/2015/10/25/Hive之分区字段的异常字符</id>
    <content type="html">1. 问题描述

hive支持动态分区技术，可以将SELECT出来的数据根据分区字段的取值动态的插入到目标分区中。例如：
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

INSERT OVERWRITE TABLE target_table_name PARTITION(partition_field_name)
SELECT
  ...
  partition_field_name
FROM source_table_name
WHERE ...

注意，使用动...</content>
  </entry>
  
  <entry>
    <title>Hive:JOIN及JOIN优化</title>
    <link href="/2015/10/25/Hive%E4%B9%8BJOIN%E5%8F%8AJOIN%E4%BC%98%E5%8C%96/"/>
    <updated>2015-10-25T00:00:00+08:00</updated>
    <id>/2015/10/25/Hive之JOIN及JOIN优化</id>
    <content type="html">1. Join的基本原理

大家都知道，Hive会将所有的SQL查询转化为Map/Reduce作业运行于Hadoop集群之上。在这里简要介绍Hive将Join转化为Map/Reduce的基本原理（其它查询的原理请参考这里）。

假定有user和order两张表，分别如下：

user表：



sid
name



1
apple


2
orange



order表：



uid
orderid



1
1001


1
1002


2
1003



现在想做student和sc两张表上的连接操作：
SELECT
  u.name,
  o.orderid
FROM us...</content>
  </entry>
  
  <entry>
    <title>OLAP</title>
    <link href="/2015/10/16/olap/"/>
    <updated>2015-10-16T00:00:00+08:00</updated>
    <id>/2015/10/16/olap</id>
    <content type="html">1.介绍

OLAP（On-Line Analytical Processing），译为联机分析处理，权威的定义为：OLAP是使分析人员、管理人员或执行人员能够从多种角度对从原始数据中转化出来的、能够真正为用户所理解的、并真实反映企业维特性的信息进行快速、一致、交互地存取，从而获得对数据的更深入了解的一类软件技术。

目前数据处理大致可以分为两大类：OLAP和OLTP(On-Line Transaction Processing，联机事务处理)。OLTP的出现早于OLAP。

OLTP是传统数据库的主要应用，面向当前的数据，主要是基本的、日常的事务处理。随着数据库技术的广泛应用，企业产生...</content>
  </entry>
  
  <entry>
    <title>Hive:源码解析之本地环境搭建</title>
    <link href="/2015/10/16/Hive%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <updated>2015-10-16T00:00:00+08:00</updated>
    <id>/2015/10/16/Hive源码解析之本地环境搭建</id>
    <content type="html">在使用了hive一段时间后，对简单的hive基本原理和操作有了一些了解。hive构建与hadoop之上，是非常流行的数据仓库工具。hive将结构化的数据映射为数据文件存储在hdfs上，并提供SQL查询功能对数据进行检索和处理。最终，hive将SQL查询转换为MapReduce任务。

面对hive强大的功能，总有一些冲动想搞清其“庐山真面目”，弄懂其运行的基本流程和原理。于是开启了hive源码的学习之路。

在这里将学习过程中的经历和心得记录下来，希望对想学习hive的同学提供一些帮助。

如果想学习hive的源码，个人感觉最好的方式就是在本地机器上搭建hive环境，以代码跟踪的方式学习...</content>
  </entry>
  
  <entry>
    <title>Hive源码解析之Hive基本框架和执行入口[转]</title>
    <link href="/2015/10/16/Hive%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8BHive%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E5%92%8C%E6%89%A7%E8%A1%8C%E5%85%A5%E5%8F%A3/"/>
    <updated>2015-10-16T00:00:00+08:00</updated>
    <id>/2015/10/16/Hive源码解析之Hive基本框架和执行入口</id>
    <content type="html">原文：http://segmentfault.com/a/1190000002766035

1.Hive简介

在介绍Hive的框架和执行流程之前，这里首先对Hive进行简要的介绍。

Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据抽取，转化，加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为Hive QL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建...</content>
  </entry>
  
  <entry>
    <title>Jekyll格式说明!</title>
    <link href="/2015/10/14/hello-jekyll/"/>
    <updated>2015-10-14T00:00:00+08:00</updated>
    <id>/2015/10/14/hello-jekyll</id>
    <content type="html">1. 标题与文字格式

标题
# 测试 h1
## 测试 h2
### 测试 h3
#### 测试 h4
##### 测试 h5
###### 测试 h6

效果：

测试 h1

测试 h2

测试 h3

测试 h4

测试 h5

测试 h6

文字格式
**这是文字粗体格式**
*这是文字斜体格式*
~~在文字上添加删除线~~

这是文字粗体格式
这是文字斜体格式
在文字上添加删除线

2. 列表

无序列表
* 项目1
* 项目2
* 项目3


项目1
项目2
项目3


有序列表
1. 项目1
2. 项目2
3. 项目3
   * 项目1
   * 项目2


项目1
项目...</content>
  </entry>
  
</feed>
